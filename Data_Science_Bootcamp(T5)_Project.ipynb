{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Science Bootcamp(T5) Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBRWkSShcT7R"
      },
      "source": [
        "# **Libraries importing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqE6njEKNnde",
        "outputId": "55755ff4-fd12-48fb-d687-0ad6626cd9ed"
      },
      "source": [
        "# CAMeL\n",
        "#If it was not the first time...\n",
        "%pip install camel-tools\n",
        "\n",
        "import os\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "os.environ['CAMELTOOLS_DATA'] = '/gdrive/MyDrive/camel_tools'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting camel-tools\n",
            "  Downloading camel_tools-1.2.0.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.15.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.6.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.7/dist-packages (from camel-tools) (4.2.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.22.2.post1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.3.4)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.9.0+cu111)\n",
            "Collecting transformers>=3.0.2\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 13.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from camel-tools) (2.23.0)\n",
            "Collecting camel-kenlm\n",
            "  Downloading camel-kenlm-2020.11.2.tar.gz (250 kB)\n",
            "\u001b[K     |████████████████████████████████| 250 kB 51.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->camel-tools) (3.7.4.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (4.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (3.3.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 50.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 75.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (21.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=3.0.2->camel-tools) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=3.0.2->camel-tools) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->camel-tools) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->camel-tools) (2.8.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->camel-tools) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->camel-tools) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->camel-tools) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->camel-tools) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.2->camel-tools) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.2->camel-tools) (7.1.2)\n",
            "Building wheels for collected packages: camel-tools, camel-kenlm\n",
            "  Building wheel for camel-tools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for camel-tools: filename=camel_tools-1.2.0-py3-none-any.whl size=99047 sha256=c3e2c05d8f5781f2ff7dd4439d44bdea94b26551afa927c6a277dd8711f406aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/ad/a1/e8aa569c102f0b8b3522ae515b7d0696046e4490c0ff4edb0a\n",
            "  Building wheel for camel-kenlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for camel-kenlm: filename=camel_kenlm-2020.11.2-cp37-cp37m-linux_x86_64.whl size=2313861 sha256=b90c5a179a1ce32f5f90664265db55837eb835d3e1143c71765e5c20b7b0ff4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/be/67/7122f437e5a4328499c80d9b4b5b6a064a3501cf24d3414087\n",
            "Successfully built camel-tools camel-kenlm\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, camel-kenlm, camel-tools\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed camel-kenlm-2020.11.2 camel-tools-1.2.0 huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n",
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB4xrE9U1V5B"
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tCCXZxvaPDL"
      },
      "source": [
        "#Orthographic Normalization \n",
        "from camel_tools.utils.normalize import normalize_alef_maksura_ar #Without needing to use regex nor indicating what letter to replace with what other letter, thereby consistency in datasets\n",
        "from camel_tools.utils.normalize import normalize_alef_ar\n",
        "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
        "from camel_tools.utils.dediac import  dediac_ar\n",
        "\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
        "\n",
        "#Orthographic Normalization \n",
        "from camel_tools.utils.normalize import normalize_alef_maksura_ar #Without needing to use regex nor indicating what letter to replace with what other letter, thereby consistency in datasets\n",
        "from camel_tools.utils.normalize import normalize_alef_ar\n",
        "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
        "from camel_tools.utils.dediac import  dediac_ar\n",
        "\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "from camel_tools.tokenizers.morphological import MorphologicalTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62ESbwpwJVtr",
        "outputId": "862722f2-560a-485e-fe91-41ec6e837e86"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "## for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import wordcloud\n",
        "## for sentiment\n",
        "from textblob import TextBlob\n",
        "import csv\n",
        "from io import open\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "from sklearn.svm import LinearSVC\n",
        "import gensim\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "from sklearn_pandas import DataFrameMapper\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "## for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import wordcloud\n",
        "## for sentiment\n",
        "from textblob import TextBlob\n",
        "import csv\n",
        "from io import open\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "from sklearn.svm import LinearSVC\n",
        "import gensim\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "from sklearn_pandas import DataFrameMapper\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "from sklearn_pandas import DataFrameMapper\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "from keras.models import Model ,Sequential\n",
        "from keras.layers import Embedding ,Dense ,Dropout , LSTM ,Input ,Activation , Bidirectional ,GlobalMaxPool1D\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import glorot_uniform\n",
        "%matplotlib inline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "\n",
        "import nltk\n",
        "import gensim\n",
        "nltk.download('punkt')\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas, xgboost, numpy, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3EoovhaZenq"
      },
      "source": [
        "#Mount drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "%mkdir /gdrive/MyDrive/camel_tools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXkesbpBio_y"
      },
      "source": [
        "data=  pd.read_excel (r'/content/drive/MyDrive/SDAIA T5/Project 1/Tweets.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWiFdGZSWJ26"
      },
      "source": [
        "##**2. Text Cleaing and Pre-Processing:**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "nuZl3Ec_yXAK",
        "outputId": "b8d4f685-9dad-43d2-f2ba-2469b71cd5c7"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neg</td>\n",
              "      <td>اعترف ان بتس كانو شوي شوي يجيبو راسي لكن اليوم...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neg</td>\n",
              "      <td>توقعت اذا جات داريا بشوفهم كاملين بس لي للحين ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>neg</td>\n",
              "      <td>#الاهلي_الهلال اكتب توقعك لنتيجة لقاء الهلال و...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>neg</td>\n",
              "      <td>نعمة المضادات الحيوية . تضع قطرة💧مضاد بنسلين ع...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>neg</td>\n",
              "      <td>الدودو جايه تكمل علي 💔</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45270</th>\n",
              "      <td>pos</td>\n",
              "      <td>السحب الليلة على الايفون .. رتويت للمرفقة وطبق...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45271</th>\n",
              "      <td>pos</td>\n",
              "      <td>😂 لابسة احمر ليه يا ست انتي ايه المناسبة 😂</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45272</th>\n",
              "      <td>pos</td>\n",
              "      <td>كلاام جمييل تستاهل(من احبه الله جعل محبته ف قل...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45273</th>\n",
              "      <td>pos</td>\n",
              "      <td>- ألطف صورة ممكن تعبر عن رمضان 💙</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45274</th>\n",
              "      <td>pos</td>\n",
              "      <td>🌸 قال #الإمام_ابن_القيم -رحمه الله تعالى- : - ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45275 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Category                                              Tweet\n",
              "0          neg  اعترف ان بتس كانو شوي شوي يجيبو راسي لكن اليوم...\n",
              "1          neg  توقعت اذا جات داريا بشوفهم كاملين بس لي للحين ...\n",
              "2          neg  #الاهلي_الهلال اكتب توقعك لنتيجة لقاء الهلال و...\n",
              "3          neg  نعمة المضادات الحيوية . تضع قطرة💧مضاد بنسلين ع...\n",
              "4          neg                             الدودو جايه تكمل علي 💔\n",
              "...        ...                                                ...\n",
              "45270      pos  السحب الليلة على الايفون .. رتويت للمرفقة وطبق...\n",
              "45271      pos         😂 لابسة احمر ليه يا ست انتي ايه المناسبة 😂\n",
              "45272      pos  كلاام جمييل تستاهل(من احبه الله جعل محبته ف قل...\n",
              "45273      pos                   - ألطف صورة ممكن تعبر عن رمضان 💙\n",
              "45274      pos  🌸 قال #الإمام_ابن_القيم -رحمه الله تعالى- : - ...\n",
              "\n",
              "[45275 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOcpsLlhocsD"
      },
      "source": [
        "# The first step is to subject the data to preprocessing.\n",
        "# This involves removing both arabic and english punctuation\n",
        "# Normalizing different letter variants with one common letter\n",
        "\n",
        "# first we define a list of arabic punctiations that we want to get rid of in our text\n",
        "punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation\n",
        "\n",
        "# Arabic stop words with nltk\n",
        "stop_words = stopwords.words()\n",
        "\n",
        "arabic_diacritics = re.compile(\"\"\"\n",
        "                             ّ    | # Shadda\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi-bpn7hURhN"
      },
      "source": [
        "def cleaing (text):\n",
        "\n",
        "  # 1.removing extra spaces\n",
        "  text = re.sub(\"s+\",\" \", text)\n",
        "\n",
        "  # 2.remove repeating char\n",
        "  text= re.sub(r'(.)\\1+', r'\\1', text)  \n",
        "\n",
        "\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcaAhINsHOG8"
      },
      "source": [
        "def preprocess(text):\n",
        "  \n",
        "    # 0. cleaing\n",
        "    text= cleaing(text)\n",
        "    \n",
        "    # 1.remove punctuations\n",
        "    translator = str.maketrans('', '', punctuations)\n",
        "    text = text.translate(translator)\n",
        "    \n",
        "    # 2.remove Tashkeel\n",
        "    #Diacritization (by using CAMeL Tools)\n",
        "    text= dediac_ar(text)\n",
        "    \n",
        "    # 3.Normalize\n",
        "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"[يى]\", \"ي\", text)\n",
        "    text = re.sub(\"[ئؤ]\", \"ء\", text)\n",
        "\n",
        "    # Normalize (by using CAMeL Tools)\n",
        "    text= normalize_alef_ar(text)\n",
        "    text= normalize_alef_maksura_ar(text)\n",
        "    text=normalize_teh_marbuta_ar(text)\n",
        "\n",
        "\n",
        "    # 4.remove stop words\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "\n",
        "    # 5.remove numbers\n",
        "    text = re.sub(\"\\d\",\"\", text)\n",
        "\n",
        "    # 0. cleaing\n",
        "    text= cleaing(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def Stemming(text):\n",
        "  text= ISRIStemmer().stem(text)\n",
        "  return text\n",
        "\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bdwqf9s2irbN"
      },
      "source": [
        "data['Tweet']=data['Tweet'].apply(str)\n",
        "data['Tweet'] = data['Tweet'].apply(Stemming)\n",
        "#data['Text'] = data['Text'].apply(preprocess)\n",
        "i=0 \n",
        "while(i<len(data)):\n",
        "  data['Tweet'][i]=preprocess(data['Tweet'][i])\n",
        " \n",
        "  i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgDJixWGO3go",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "8b3df59f-cb1d-4475-915c-ae6d459c2c5c"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "data.to_excel(\"cleanTweets.xlsx\", index=False, encoding='utf8')\n",
        "\n",
        "files.download(\"cleanTweets.xlsx\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_2484bf6b-3d8c-44de-984c-9e8cba3179c5\", \"cleanTweets.xlsx\", 3010990)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkxqjGV2uvnv"
      },
      "source": [
        "data =pd.read_excel(\"/content/drive/MyDrive/SDAIA T5/Project 1/cleanTweets.xlsx\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZzydggs_4wz"
      },
      "source": [
        "#####  After we build some models we noticed the accuracy was better than f1-score so we expected the data was imbalanced let's make sure:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4UuQ2le_7iI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "9eb82c87-2816-4fb7-8d8b-a1586385a351"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.catplot(x=\"Category\", kind=\"count\", palette=\"cool\", data=data,height=5, aspect=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<seaborn.axisgrid.FacetGrid at 0x7fdc22c3aed0>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 3600x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48RHT69YApvl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af95815c-9525-426c-ecc3-53c7698eccc1"
      },
      "source": [
        "data[\"Category\"].value_counts()  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pos    22761\n",
              "neg    22514\n",
              "Name: Category, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMiqacFfSF6K"
      },
      "source": [
        "##**1. Feature Engineering:**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGDlfBGA3_7Q"
      },
      "source": [
        "word = data[\"Tweet\"].apply(lambda x: len(str(x).split(\" \")))\n",
        "data['char_count'] = data[\"Tweet\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\n",
        "sentence = data[\"Tweet\"].apply(lambda x: len(str(x).split(\".\")))\n",
        "data['word_density'] = data['char_count'] / word\n",
        "data['sentence_density'] = word / sentence\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75pJmIUjSpVv"
      },
      "source": [
        "i=0\n",
        "data['Tweet']=data['Tweet'].apply(str)\n",
        "while i<len(data):\n",
        "  data['count'][i]= data['Tweet'][i].count('#')\n",
        "  \n",
        "  i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "VM7Td9IXT-X7",
        "outputId": "b0ee2326-5702-44c3-cc84-ec162a77cf90"
      },
      "source": [
        "positiveList=['الحمد','حل','العافيه','مبروك','اجمل','بالتوفيق','تفاءل','النصر','التعاون','صح','جميل','افضل','شكرا','سعاده','ايجابي','الخير','النجاح','اكثر','راءع','متاز']\n",
        "NegativeList=['اقل','سيءه','عدم','المشكله','اعتداء','الهم','ضد','اهمال','جراح','خاب','غبي','محزن','سلبي','مءلم','فاشل','سيء','لاسف','خطا']\n",
        "\n",
        "PosWords=[]\n",
        "NegWords= []\n",
        "i=0\n",
        "while i<len(data):\n",
        "  words= data['Tweet'][i].split()\n",
        "  pos=0\n",
        "  neg=0\n",
        "  for w in words:\n",
        "    if(w in positiveList):\n",
        "      pos+=1\n",
        "    if(w in NegativeList):\n",
        "      neg+=1\n",
        "  PosWords.append(pos)\n",
        "  NegWords.append(neg)\n",
        "  i+=1\n",
        "\n",
        "data['PosWords']= PosWords\n",
        "data['NegWords']= NegWords\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>char_count</th>\n",
              "      <th>word_density</th>\n",
              "      <th>sentence_density</th>\n",
              "      <th>#count</th>\n",
              "      <th>PosWords</th>\n",
              "      <th>NegWords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neg</td>\n",
              "      <td>اعترف ان بتس كانو شوي شوي يجيبو راسي اليوم بال...</td>\n",
              "      <td>42</td>\n",
              "      <td>3.818182</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neg</td>\n",
              "      <td>توقعت اذا جات داريا بشوفهم كاملين لحين احس احد...</td>\n",
              "      <td>49</td>\n",
              "      <td>4.083333</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>neg</td>\n",
              "      <td>اهليالهلال اكتب توقعك لنتيجه لقاء الهلال والاه...</td>\n",
              "      <td>82</td>\n",
              "      <td>5.125000</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>neg</td>\n",
              "      <td>نعمه المضادات الحيويه تضع قطره💧مضاد بنسلين علي...</td>\n",
              "      <td>91</td>\n",
              "      <td>5.055556</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>neg</td>\n",
              "      <td>دودو جايه تكمل علي 💔</td>\n",
              "      <td>16</td>\n",
              "      <td>3.200000</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45270</th>\n",
              "      <td>pos</td>\n",
              "      <td>سحب اليله علي الايفون رتويت لمرفقه وطبق الشروط 👇</td>\n",
              "      <td>40</td>\n",
              "      <td>4.444444</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45271</th>\n",
              "      <td>pos</td>\n",
              "      <td>😂 لابسه احمر ليه انتي ايه المناسبه 😂</td>\n",
              "      <td>29</td>\n",
              "      <td>3.625000</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45272</th>\n",
              "      <td>pos</td>\n",
              "      <td>كلام جميل تستاهلمن احبه اله محبته قلوب البشر 💙</td>\n",
              "      <td>38</td>\n",
              "      <td>4.222222</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45273</th>\n",
              "      <td>pos</td>\n",
              "      <td>الطف صوره مكن تعبر رمضان 💙</td>\n",
              "      <td>21</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45274</th>\n",
              "      <td>pos</td>\n",
              "      <td>🌸 قال الامامابنالقيم رحمه اله تعالي فان ير نعم...</td>\n",
              "      <td>70</td>\n",
              "      <td>4.375000</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45275 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Category  ... NegWords\n",
              "0          neg  ...        0\n",
              "1          neg  ...        0\n",
              "2          neg  ...        0\n",
              "3          neg  ...        0\n",
              "4          neg  ...        0\n",
              "...        ...  ...      ...\n",
              "45270      pos  ...        0\n",
              "45271      pos  ...        0\n",
              "45272      pos  ...        0\n",
              "45273      pos  ...        0\n",
              "45274      pos  ...        0\n",
              "\n",
              "[45275 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DunCH_nSRi21"
      },
      "source": [
        "# **- Text Embedding**:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8HYTXxK3RSr"
      },
      "source": [
        "## 1. AraVec "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdaWTsgYpHkB",
        "outputId": "6d2e3ffb-b59b-440e-f85d-a3952bf3252d"
      },
      "source": [
        "!pip install gensim spacy nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqh5uyKG7D3u",
        "outputId": "aed46881-27b2-4138-949c-9a8aa42a1ad9"
      },
      "source": [
        "# load the AraVec model\n",
        "model = gensim.models.Word2Vec.load(\"/content/drive/MyDrive/NLP_Project/AraVec/www_cbow_100\")\n",
        "print(\"We've\",len(model.wv.index2word),\"vocabularies\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We've 234961 vocabularies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tolYc8zz7mNS",
        "outputId": "77827169-d3dc-48ee-b1fb-b083cac05815"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBluBlIS7D6w"
      },
      "source": [
        "# make a directory called \"spacyModel\"\n",
        "%mkdir spacyModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEcICqhF7D--"
      },
      "source": [
        "# export the word2vec fomart to the directory\n",
        "model.wv.save_word2vec_format(\"./spacyModel/aravec.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-PvGc0VOBZF"
      },
      "source": [
        "# using `gzip` to compress the .txt file\n",
        "!gzip ./spacyModel/aravec.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3YN2muJOFoq",
        "outputId": "7aa35597-e4a2-41aa-fa82-b6851e77b964"
      },
      "source": [
        "!python -m spacy  init-model ar spacy.aravec.model --vectors-loc ./spacyModel/aravec.txt.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K\u001b[38;5;2m✔ Successfully created model\u001b[0m\n",
            "234961it [00:10, 23282.34it/s]\n",
            "\u001b[2K\u001b[38;5;2m✔ Loaded vectors from spacyModel/aravec.txt.gz\u001b[0m\n",
            "\u001b[38;5;2m✔ Sucessfully compiled vocab\u001b[0m\n",
            "235153 entries, 234961 vectors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN2iOvglOW91"
      },
      "source": [
        "# load AraVec Spacy model\n",
        "nlp = spacy.load(\"./spacy.aravec.model/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDCxm3SLOZXu"
      },
      "source": [
        "# Define the preprocessing Class\n",
        "class Preprocessorr:\n",
        "    def __init__(self, tokenizer, **cfg):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, text):\n",
        "        preprocessed = preprocess(text)\n",
        "        return self.tokenizer(preprocessed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cehb0GXVPNOl"
      },
      "source": [
        "#Apply the `Preprocessor` Class\n",
        "nlp.tokenizer = Preprocessorr(nlp.tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJkodaOHwz5R",
        "outputId": "7f13a1ab-171e-4850-ed2d-e81d8388a81e"
      },
      "source": [
        "AraVec_word_embedding = np.array([nlp(d).vector for d in data['Tweet'].values.astype('U')])\n",
        "\n",
        "# print the dimension of x      \n",
        "print(AraVec_word_embedding.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(45275, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx1IMi9gPhQt"
      },
      "source": [
        "## 3. N-gram  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThYSnMAnjIdK"
      },
      "source": [
        "texts= np.array([d for d in data['Tweet']])\n",
        "\n",
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(data['Tweet'].astype('U').values)\n",
        "Ngrame_word_embedding =  tfidf_vect_ngram.transform(texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMHN2jovP_Fi"
      },
      "source": [
        "## 3. Tf-idf  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IX4zz4DRVSd"
      },
      "source": [
        "vect = TfidfVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx9tG8Q9TIQW"
      },
      "source": [
        "texts= np.array([d for d in data['Tweet']])\n",
        "TfIdf_word_embedding = vect.fit_transform(texts) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBEOeF-KTOrM"
      },
      "source": [
        "# **4. Modeling**:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLUUE0v5QbS9"
      },
      "source": [
        "# label encode the target variable \n",
        "encoder = preprocessing.LabelEncoder()\n",
        "data['encoded'] = encoder.fit_transform(data['Category'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8_0OQivf78R"
      },
      "source": [
        "y = np.array([d for d in data['encoded']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzWOwTY_Vjys"
      },
      "source": [
        "# function to train and test model: take 5 parameters model name, training and testing data. it return the evaluation report\n",
        "def building_model(classifier, X_train, y_train, X_test, y_test):\n",
        "\n",
        "    # fit the training dataset on the classifier\n",
        "    classifier.fit(X_train, y_train)\n",
        "\n",
        "    # predict the labels on validation dataset\n",
        "    prediction = dict()\n",
        "    prediction[\"lr\"] = classifier.predict(X_test)\n",
        "\n",
        "    # report\n",
        "    report= classification_report(y_test,prediction[\"lr\"], output_dict=True)\n",
        "    report2 = classification_report(y_test, prediction[\"lr\"], labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,40],  output_dict=True)\n",
        "    \n",
        "    Precision_macro= report['macro avg']['precision'] * 100\n",
        "    Recall_macro= report['macro avg']['recall'] * 100\n",
        "    f1_macro= report['macro avg']['f1-score'] * 100\n",
        "    Accuracy= report['accuracy'] * 100\n",
        "\n",
        "    Precision_micro= report2['micro avg']['precision'] * 100\n",
        "    Recall_micro= report2['micro avg']['recall'] * 100\n",
        "    f1_micro= report2['micro avg']['f1-score']  * 100\n",
        "\n",
        "    Precision_weighted = report['weighted avg']['precision'] * 100\n",
        "    Recall_weighted = report['weighted avg']['recall'] * 100\n",
        "    f1_weighted = report['weighted avg']['f1-score'] * 100\n",
        "    \n",
        "     \n",
        "\n",
        "    return [Accuracy, Precision_macro,Recall_macro, f1_macro, Precision_micro, Recall_micro,f1_micro,Precision_weighted,Recall_weighted, f1_weighted]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8jNUCdaVAD-"
      },
      "source": [
        "## 4.1. Logistic Regrssion: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2sWSmCDJ7i9"
      },
      "source": [
        "classifier = linear_model.LogisticRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icmGxvgTXvE9"
      },
      "source": [
        "###### 4.1.1. Logistic Regression and Aravec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIEaN5LgbUmo",
        "outputId": "301d616b-0a87-47ca-9a40-76c020298a3b"
      },
      "source": [
        "# Without Other Features\n",
        "x = AraVec_word_embedding\n",
        "#------------------------------------\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(x,y, test_size = 0.2, random_state = 10) #Splitting data into training and testing\n",
        "#------------------------------------\n",
        "\n",
        "report = building_model(classifier, X_train, y_train, X_test, y_test)\n",
        "\n",
        "print ('Accuracy : ',report[0])\n",
        "print ('-----------------------')\n",
        "print ('Precision (macro avg) : ',report[1])\n",
        "print ('Recall (macro avg)   : ',report[2])\n",
        "print ('F-score (macro avg)  : ',report[3])\n",
        "print ('-----------------------')\n",
        "print ('Precision (micro avg) : ',report[4])\n",
        "print ('Recall (micro avg)   : ',report[5])\n",
        "print ('F-score (micro avg)  : ',report[6])\n",
        "print ('-----------------------')\n",
        "print ('Precision (weighted avg) : ',report[7])\n",
        "print ('Recall (weighted avg)   : ',report[8])\n",
        "print ('F-score (weighted avg)  : ',report[9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy :  67.23357261181667\n",
            "-----------------------\n",
            "Precision (macro avg) :  67.2803342944406\n",
            "Recall (macro avg)   :  67.2466530969519\n",
            "F-score (macro avg)  :  67.22118916280387\n",
            "-----------------------\n",
            "Precision (micro avg) :  67.23357261181667\n",
            "Recall (micro avg)   :  67.23357261181667\n",
            "F-score (micro avg)  :  67.23357261181667\n",
            "-----------------------\n",
            "Precision (weighted avg) :  67.28625202598855\n",
            "Recall (weighted avg)   :  67.23357261181667\n",
            "F-score (weighted avg)  :  67.21760077701039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPUXMIsYYYnS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d38180f-a4d2-423f-e183-39b2d0a2c7c8"
      },
      "source": [
        "# With Other Features\n",
        "features= data.loc[:, [ \"word_density\", \"sentence_density\",\"char_count\",\"PosWords\", \"NegWords\",\"word_count\"]].values\n",
        "x_Features= np.concatenate([x, features], axis=1)\n",
        "#------------------------------------\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(x_Features,y, test_size = 0.2, random_state = 10) #Splitting data into training and testing\n",
        "#------------------------------------\n",
        "\n",
        "report = building_model(classifier, X_train, y_train, X_test, y_test)\n",
        "\n",
        "print ('Accuracy : ',report[0])\n",
        "print ('-----------------------')\n",
        "print ('Precision (macro avg) : ',report[1])\n",
        "print ('Recall (macro avg)   : ',report[2])\n",
        "print ('F-score (macro avg)  : ',report[3])\n",
        "print ('-----------------------')\n",
        "print ('Precision (micro avg) : ',report[4])\n",
        "print ('Recall (micro avg)   : ',report[5])\n",
        "print ('F-score (micro avg)  : ',report[6])\n",
        "print ('-----------------------')\n",
        "print ('Precision (weighted avg) : ',report[7])\n",
        "print ('Recall (weighted avg)   : ',report[8])\n",
        "print ('F-score (weighted avg)  : ',report[9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy :  66.70347874102706\n",
            "-----------------------\n",
            "Precision (macro avg) :  66.71105046236536\n",
            "Recall (macro avg)   :  66.70808288538672\n",
            "F-score (macro avg)  :  66.7028289848603\n",
            "-----------------------\n",
            "Precision (micro avg) :  66.70347874102706\n",
            "Recall (micro avg)   :  66.70347874102706\n",
            "F-score (micro avg)  :  66.70347874102706\n",
            "-----------------------\n",
            "Precision (weighted avg) :  66.71399823174134\n",
            "Recall (weighted avg)   :  66.70347874102706\n",
            "F-score (weighted avg)  :  66.7020005457477\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubbcptappKKP"
      },
      "source": [
        "###### 4.1.2. Logistic Regression and N-gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4xJjFq9pKKn",
        "outputId": "ef91a1df-3b1f-4424-e713-28257ccb26eb"
      },
      "source": [
        "# Without Other Features\n",
        "x = Ngrame_word_embedding\n",
        "#------------------------------------\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(x,y, test_size = 0.2, random_state = 10) #Splitting data into training and testing\n",
        "#------------------------------------\n",
        "\n",
        "report = building_model(classifier, X_train, y_train, X_test, y_test)\n",
        "\n",
        "print ('Accuracy : ',report[0])\n",
        "print ('-----------------------')\n",
        "print ('Precision (macro avg) : ',report[1])\n",
        "print ('Recall (macro avg)   : ',report[2])\n",
        "print ('F-score (macro avg)  : ',report[3])\n",
        "print ('-----------------------')\n",
        "print ('Precision (micro avg) : ',report[4])\n",
        "print ('Recall (micro avg)   : ',report[5])\n",
        "print ('F-score (micro avg)  : ',report[6])\n",
        "print ('-----------------------')\n",
        "print ('Precision (weighted avg) : ',report[7])\n",
        "print ('Recall (weighted avg)   : ',report[8])\n",
        "print ('F-score (weighted avg)  : ',report[9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy :  64.6604086140254\n",
            "-----------------------\n",
            "Precision (macro avg) :  73.26841213570809\n",
            "Recall (macro avg)   :  64.83046849471104\n",
            "F-score (macro avg)  :  61.20894019333198\n",
            "-----------------------\n",
            "Precision (micro avg) :  64.6604086140254\n",
            "Recall (micro avg)   :  64.6604086140254\n",
            "F-score (micro avg)  :  64.6604086140254\n",
            "-----------------------\n",
            "Precision (weighted avg) :  73.34891977913477\n",
            "Recall (weighted avg)   :  64.6604086140254\n",
            "F-score (weighted avg)  :  61.14376992696569\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZXEB-KmU9Bt"
      },
      "source": [
        "###### 4.1.3. Logistic Regression and Tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w38UvEtXVHOO",
        "outputId": "d6b10b7d-4a28-40d1-b053-07727385a50d"
      },
      "source": [
        "# Without Other Features\n",
        "x = TfIdf_word_embedding\n",
        "#------------------------------------\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(x,y, test_size = 0.2, random_state = 10) #Splitting data into training and testing\n",
        "#------------------------------------\n",
        "\n",
        "report = building_model(classifier, X_train, y_train, X_test, y_test)\n",
        "\n",
        "print ('Accuracy : ',report[0])\n",
        "print ('-----------------------')\n",
        "print ('Precision (macro avg) : ',report[1])\n",
        "print ('Recall (macro avg)   : ',report[2])\n",
        "print ('F-score (macro avg)  : ',report[3])\n",
        "print ('-----------------------')\n",
        "print ('Precision (micro avg) : ',report[4])\n",
        "print ('Recall (micro avg)   : ',report[5])\n",
        "print ('F-score (micro avg)  : ',report[6])\n",
        "print ('-----------------------')\n",
        "print ('Precision (weighted avg) : ',report[7])\n",
        "print ('Recall (weighted avg)   : ',report[8])\n",
        "print ('F-score (weighted avg)  : ',report[9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy :  77.30535615681944\n",
            "-----------------------\n",
            "Precision (macro avg) :  77.48691464292993\n",
            "Recall (macro avg)   :  77.32771329490869\n",
            "F-score (macro avg)  :  77.27697767601602\n",
            "-----------------------\n",
            "Precision (micro avg) :  77.30535615681944\n",
            "Recall (micro avg)   :  77.30535615681944\n",
            "F-score (micro avg)  :  77.30535615681943\n",
            "-----------------------\n",
            "Precision (weighted avg) :  77.50031476274417\n",
            "Recall (weighted avg)   :  77.30535615681944\n",
            "F-score (weighted avg)  :  77.27245485563799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6R5faReV5sV"
      },
      "source": [
        "## 4.2. KNN: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdQG5E-7V5sW"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors =40, metric = 'minkowski', p = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rdtgk4mYV5sX"
      },
      "source": [
        "###### 4.2.1. KNN and Aravec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzWNpVEUV5sX",
        "outputId": "3f11ed4d-e05f-4700-9904-8df7e930dd05"
      },
      "source": [
        "# Without Other Features\n",
        "x = AraVec_word_embedding\n",
        "#------------------------------------\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(x,y, test_size = 0.2, random_state = 10) #Splitting data into training and testing\n",
        "#------------------------------------\n",
        "\n",
        "report = building_model(classifier, X_train, y_train, X_test, y_test)\n",
        "\n",
        "print ('Accuracy : ',report[0])\n",
        "print ('-----------------------')\n",
        "print ('Precision (macro avg) : ',report[1])\n",
        "print ('Recall (macro avg)   : ',report[2])\n",
        "print ('F-score (macro avg)  : ',report[3])\n",
        "print ('-----------------------')\n",
        "print ('Precision (micro avg) : ',report[4])\n",
        "print ('Recall (micro avg)   : ',report[5])\n",
        "print ('F-score (micro avg)  : ',report[6])\n",
        "print ('-----------------------')\n",
        "print ('Precision (weighted avg) : ',report[7])\n",
        "print ('Recall (weighted avg)   : ',report[8])\n",
        "print ('F-score (weighted avg)  : ',report[9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy :  69.91717283268912\n",
            "-----------------------\n",
            "Precision (macro avg) :  69.99618665165602\n",
            "Recall (macro avg)   :  69.93363517671283\n",
            "F-score (macro avg)  :  69.89792018079832\n",
            "-----------------------\n",
            "Precision (micro avg) :  69.91717283268912\n",
            "Recall (micro avg)   :  69.91717283268912\n",
            "F-score (micro avg)  :  69.91717283268912\n",
            "-----------------------\n",
            "Precision (weighted avg) :  70.00410355689631\n",
            "Recall (weighted avg)   :  69.91717283268912\n",
            "F-score (weighted avg)  :  69.89363247229862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFidyG7DV5sm"
      },
      "source": [
        "###### 4.2.2. KNN and N-gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn7MBXlCV5sm",
        "outputId": "a7f3a745-8df0-4b5d-ddef-65ab90a4cae1"
      },
      "source": [
        "# Without Other Features\n",
        "x = Ngrame_word_embedding\n",
        "#------------------------------------\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(x,y, test_size = 0.2, random_state = 10) #Splitting data into training and testing\n",
        "#------------------------------------\n",
        "\n",
        "report = building_model(classifier, X_train, y_train, X_test, y_test)\n",
        "\n",
        "print ('Accuracy : ',report[0])\n",
        "print ('-----------------------')\n",
        "print ('Precision (macro avg) : ',report[1])\n",
        "print ('Recall (macro avg)   : ',report[2])\n",
        "print ('F-score (macro avg)  : ',report[3])\n",
        "print ('-----------------------')\n",
        "print ('Precision (micro avg) : ',report[4])\n",
        "print ('Recall (micro avg)   : ',report[5])\n",
        "print ('F-score (micro avg)  : ',report[6])\n",
        "print ('-----------------------')\n",
        "print ('Precision (weighted avg) : ',report[7])\n",
        "print ('Recall (weighted avg)   : ',report[8])\n",
        "print ('F-score (weighted avg)  : ',report[9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy :  63.90944229707344\n",
            "-----------------------\n",
            "Precision (macro avg) :  71.94246929196987\n",
            "Recall (macro avg)   :  64.07848311651614\n",
            "F-score (macro avg)  :  60.430280354836206\n",
            "-----------------------\n",
            "Precision (micro avg) :  63.90944229707344\n",
            "Recall (micro avg)   :  63.90944229707344\n",
            "F-score (micro avg)  :  63.90944229707344\n",
            "-----------------------\n",
            "Precision (weighted avg) :  72.01804287188865\n",
            "Recall (weighted avg)   :  63.90944229707344\n",
            "F-score (weighted avg)  :  60.3641957145926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qQVOhcMV5sn"
      },
      "source": [
        "###### 4.2.3. KNN and Tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjvoG7qBvPsU",
        "outputId": "d21e53c3-35f2-4989-ce7f-4981044883bf"
      },
      "source": [
        "# Without Other Features\n",
        "x = TfIdf_word_embedding\n",
        "#------------------------------------\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(x,y, test_size = 0.2, random_state = 10) #Splitting data into training and testing\n",
        "#------------------------------------\n",
        "\n",
        "report = building_model(classifier, X_train, y_train, X_test, y_test)\n",
        "\n",
        "print ('Accuracy : ',report[0])\n",
        "print ('-----------------------')\n",
        "print ('Precision (macro avg) : ',report[1])\n",
        "print ('Recall (macro avg)   : ',report[2])\n",
        "print ('F-score (macro avg)  : ',report[3])\n",
        "print ('-----------------------')\n",
        "print ('Precision (micro avg) : ',report[4])\n",
        "print ('Recall (micro avg)   : ',report[5])\n",
        "print ('F-score (micro avg)  : ',report[6])\n",
        "print ('-----------------------')\n",
        "print ('Precision (weighted avg) : ',report[7])\n",
        "print ('Recall (weighted avg)   : ',report[8])\n",
        "print ('F-score (weighted avg)  : ',report[9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy :  57.84649364991717\n",
            "-----------------------\n",
            "Precision (macro avg) :  74.14543003718907\n",
            "Recall (macro avg)   :  58.076487078539806\n",
            "F-score (macro avg)  :  49.57172507292621\n",
            "-----------------------\n",
            "Precision (micro avg) :  57.84649364991717\n",
            "Recall (micro avg)   :  57.84649364991717\n",
            "F-score (micro avg)  :  57.84649364991717\n",
            "-----------------------\n",
            "Precision (weighted avg) :  74.25795822938494\n",
            "Recall (weighted avg)   :  57.84649364991717\n",
            "F-score (weighted avg)  :  49.45667240187209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBT06E_ad9Su"
      },
      "source": [
        "## 4.3. Support vector machines SVM: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AM3CFEfd9S4"
      },
      "source": [
        "classifier = LinearSVC()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYQnXk8yd9S6"
      },
      "source": [
        "###### 4.3.1. SVM and Aravec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiSq3vHQd9S6",
        "outputId": "066b3267-648b-4dbc-852c-ec8a8885b52f"
      },
      "source": [
        "# Without Other Features\n",
        "x = AraVec_word_embedding\n",
        "#------------------------------------\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(x,y, test_size = 0.2, random_state = 10) #Splitting data into training and testing\n",
        "#------------------------------------\n",
        "\n",
        "report = building_model(classifier, X_train, y_train, X_test, y_test)\n",
        "\n",
        "print ('Accuracy : ',report[0])\n",
        "print ('-----------------------')\n",
        "print ('Precision (macro avg) : ',report[1])\n",
        "print ('Recall (macro avg)   : ',report[2])\n",
        "print ('F-score (macro avg)  : ',report[3])\n",
        "print ('-----------------------')\n",
        "print ('Precision (micro avg) : ',report[4])\n",
        "print ('Recall (micro avg)   : ',report[5])\n",
        "print ('F-score (micro avg)  : ',report[6])\n",
        "print ('-----------------------')\n",
        "print ('Precision (weighted avg) : ',report[7])\n",
        "print ('Recall (weighted avg)   : ',report[8])\n",
        "print ('F-score (weighted avg)  : ',report[9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy :  66.68139149641082\n",
            "-----------------------\n",
            "Precision (macro avg) :  66.83445388115409\n",
            "Recall (macro avg)   :  66.70652172746418\n",
            "F-score (macro avg)  :  66.62566790206597\n",
            "-----------------------\n",
            "Precision (micro avg) :  66.68139149641082\n",
            "Recall (micro avg)   :  66.68139149641082\n",
            "F-score (micro avg)  :  66.68139149641082\n",
            "-----------------------\n",
            "Precision (weighted avg) :  66.84432263162425\n",
            "Recall (weighted avg)   :  66.68139149641082\n",
            "F-score (weighted avg)  :  66.61798708230492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdH8_cGtd9S9"
      },
      "source": [
        "###### 4.3.2. SVM and N-gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8texeold9S9",
        "outputId": "b0586109-8324-477d-cdd6-fae41e8f3ea1"
      },
      "source": [
        "# Without Other Features\n",
        "x = Ngrame_word_embedding\n",
        "#------------------------------------\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(x,y, test_size = 0.2, random_state = 10) #Splitting data into training and testing\n",
        "#------------------------------------\n",
        "\n",
        "report = building_model(classifier, X_train, y_train, X_test, y_test)\n",
        "\n",
        "print ('Accuracy : ',report[0])\n",
        "print ('-----------------------')\n",
        "print ('Precision (macro avg) : ',report[1])\n",
        "print ('Recall (macro avg)   : ',report[2])\n",
        "print ('F-score (macro avg)  : ',report[3])\n",
        "print ('-----------------------')\n",
        "print ('Precision (micro avg) : ',report[4])\n",
        "print ('Recall (micro avg)   : ',report[5])\n",
        "print ('F-score (micro avg)  : ',report[6])\n",
        "print ('-----------------------')\n",
        "print ('Precision (weighted avg) : ',report[7])\n",
        "print ('Recall (weighted avg)   : ',report[8])\n",
        "print ('F-score (weighted avg)  : ',report[9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy :  64.71562672556598\n",
            "-----------------------\n",
            "Precision (macro avg) :  73.50407481370597\n",
            "Recall (macro avg)   :  64.8866213937374\n",
            "F-score (macro avg)  :  61.22737492091045\n",
            "-----------------------\n",
            "Precision (micro avg) :  64.71562672556598\n",
            "Recall (micro avg)   :  64.71562672556598\n",
            "F-score (micro avg)  :  64.71562672556598\n",
            "-----------------------\n",
            "Precision (weighted avg) :  73.58582036404249\n",
            "Recall (weighted avg)   :  64.71562672556598\n",
            "F-score (weighted avg)  :  61.16187387450491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ36WQI7d9S-"
      },
      "source": [
        "###### 4.3.3. SVM and Tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbAwLJaLd9S-",
        "outputId": "b160631d-f982-4d37-aa83-84196468ea3c"
      },
      "source": [
        "# Without Other Features\n",
        "x = TfIdf_word_embedding\n",
        "#------------------------------------\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(x,y, test_size = 0.2, random_state = 10) #Splitting data into training and testing\n",
        "#------------------------------------\n",
        "\n",
        "report = building_model(classifier, X_train, y_train, X_test, y_test)\n",
        "\n",
        "print ('Accuracy : ',report[0])\n",
        "print ('-----------------------')\n",
        "print ('Precision (macro avg) : ',report[1])\n",
        "print ('Recall (macro avg)   : ',report[2])\n",
        "print ('F-score (macro avg)  : ',report[3])\n",
        "print ('-----------------------')\n",
        "print ('Precision (micro avg) : ',report[4])\n",
        "print ('Recall (micro avg)   : ',report[5])\n",
        "print ('F-score (micro avg)  : ',report[6])\n",
        "print ('-----------------------')\n",
        "print ('Precision (weighted avg) : ',report[7])\n",
        "print ('Recall (weighted avg)   : ',report[8])\n",
        "print ('F-score (weighted avg)  : ',report[9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy :  77.85753727222529\n",
            "-----------------------\n",
            "Precision (macro avg) :  77.90511891387497\n",
            "Recall (macro avg)   :  77.86871549780008\n",
            "F-score (macro avg)  :  77.8522429677667\n",
            "-----------------------\n",
            "Precision (micro avg) :  77.85753727222529\n",
            "Recall (micro avg)   :  77.85753727222529\n",
            "F-score (micro avg)  :  77.85753727222529\n",
            "-----------------------\n",
            "Precision (weighted avg) :  77.91245022421978\n",
            "Recall (weighted avg)   :  77.85753727222529\n",
            "F-score (weighted avg)  :  77.85031432828534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W6ABp2IOL9I"
      },
      "source": [
        "m= classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pCjlFtpPGwM"
      },
      "source": [
        "import pickle\n",
        "# save the model to disk\n",
        "filename = 'finalized_model.sav'\n",
        "pickle.dump(classifier, open(filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhQlGR3yORye",
        "outputId": "ae0a0b6a-20c6-4170-ea59-496ab6b6d6e0"
      },
      "source": [
        "# load the model from disk\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "result = loaded_model.score(X_train, y_train)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9913576367064739\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}